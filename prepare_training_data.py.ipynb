{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6a42b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "# import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1223fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select language\n",
    "language = \"english\"\n",
    "# language = \"slovenian\"\n",
    "# language = \"romanian\"\n",
    "# language = \"persian\"\n",
    "\n",
    "# Select number of sentences for each corpus\n",
    "# num_sents = [100, 500, 1000, 2000]\n",
    "num_sents = [125, 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86826f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(language):\n",
    "    \"\"\"\n",
    "    Loads corpus files depending on selected language.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {language} corpora...\")\n",
    "    if language==\"english\":\n",
    "        # GUM (Georgetown University Multilayer) corpus from UD website\n",
    "        corpus_A_dir = \"data/ud-en-gum/\"\n",
    "        train_A = corpus_A_dir+\"en_gum-ud-train.conllu\"\n",
    "        dev_A = corpus_A_dir+\"en_gum-ud-dev.conllu\"\n",
    "\n",
    "        # wsj corpus with different conventions\n",
    "        # converted from Stanford dependencies (?)\n",
    "        corpus_B_dir = \"data/wsj-DIFF-CONVENTIONS/\"\n",
    "        train_B = corpus_B_dir+\"train.conllu\"\n",
    "        dev_B = corpus_B_dir+\"dev.conllu\"\n",
    "        test_B = corpus_B_dir+\"test.conllu\"\n",
    "    elif language == \"persian\":\n",
    "        # PerDT corpus\n",
    "        corpus_A_dir = \"data/PerDT/\"\n",
    "        train_A = corpus_A_dir+\"fa_perdt-ud-train-removeLines.conllu\"\n",
    "        dev_A = corpus_A_dir+\"fa_perdt-ud-dev-removeLines.conllu\"\n",
    "        # original seraji\n",
    "        corpus_B_dir = \"data/original_seraji/\"\n",
    "        train_B = corpus_B_dir+\"fa_seraji-ud-train-removeLines.conllu\"\n",
    "        dev_B = corpus_B_dir+\"fa_seraji-ud-dev-removeLines.conllu\"\n",
    "        # new seraji\n",
    "        corpus_C_dir = \"data/new_seraji/\"\n",
    "        train_C = corpus_C_dir+\"fa_newseraji-ud-train-removeLines.conllu\"\n",
    "        dev_C = corpus_C_dir+\"fa_newseraji-ud-dev-removeLines.conllu\"\n",
    "    else:\n",
    "        print(\"Please select a language!\")\n",
    "    print(\"Corpora loaded!\")\n",
    "\n",
    "#     return train_A, train_B#, train_C\n",
    "    return dev_A, dev_B#, dev_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fcc5c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading english corpora...\n",
      "Corpora loaded!\n"
     ]
    }
   ],
   "source": [
    "corpus_A, corpus_B = load_corpus(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69cf5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_partitions(ud_file, ab, num_sents):\n",
    "    \"\"\"\n",
    "    Processes UD training data into randomly sampled training partitions of a fixed size.\n",
    "    \"\"\"\n",
    "    with open(ud_file) as infile:\n",
    "        ud_lines = infile.readlines()\n",
    "    # get list of sentences\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in ud_lines:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elif len(line.strip()) == 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            split = line.split(\"\\t\")\n",
    "            sentence.append(split)\n",
    "    \n",
    "    for size in num_sents:\n",
    "        i = 1\n",
    "        while i <=5:\n",
    "            print(f\"Creating dev partition {i} of {size} sentences for {language} corpus {ab}.\")\n",
    "            seed_filename = \"dev_data/\"+language+\"_dev_corpus=\"+ab+\"_sents=\"+str(size)+\"_seed=\"+str(i)+\".txt\"\n",
    "            random.shuffle(sentences)\n",
    "            with open(seed_filename, \"w\") as outfile:\n",
    "                for line in sentences[:size]:\n",
    "                    for entry in line:\n",
    "                        outfile.write(\"\\t\".join(entry))\n",
    "                    outfile.write(\"\\n\")\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da2d3345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dev partition 1 of 125 sentences for english corpus A.\n",
      "Creating dev partition 2 of 125 sentences for english corpus A.\n",
      "Creating dev partition 3 of 125 sentences for english corpus A.\n",
      "Creating dev partition 4 of 125 sentences for english corpus A.\n",
      "Creating dev partition 5 of 125 sentences for english corpus A.\n",
      "Creating dev partition 1 of 250 sentences for english corpus A.\n",
      "Creating dev partition 2 of 250 sentences for english corpus A.\n",
      "Creating dev partition 3 of 250 sentences for english corpus A.\n",
      "Creating dev partition 4 of 250 sentences for english corpus A.\n",
      "Creating dev partition 5 of 250 sentences for english corpus A.\n",
      "Creating dev partition 1 of 125 sentences for english corpus B.\n",
      "Creating dev partition 2 of 125 sentences for english corpus B.\n",
      "Creating dev partition 3 of 125 sentences for english corpus B.\n",
      "Creating dev partition 4 of 125 sentences for english corpus B.\n",
      "Creating dev partition 5 of 125 sentences for english corpus B.\n",
      "Creating dev partition 1 of 250 sentences for english corpus B.\n",
      "Creating dev partition 2 of 250 sentences for english corpus B.\n",
      "Creating dev partition 3 of 250 sentences for english corpus B.\n",
      "Creating dev partition 4 of 250 sentences for english corpus B.\n",
      "Creating dev partition 5 of 250 sentences for english corpus B.\n"
     ]
    }
   ],
   "source": [
    "create_training_partitions(corpus_A, \"A\", num_sents)\n",
    "create_training_partitions(corpus_B, \"B\", num_sents)\n",
    "# create_training_partitions(corpus_C, \"C\", num_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647ceef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d5264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
