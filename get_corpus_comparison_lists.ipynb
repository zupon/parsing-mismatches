{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
<<<<<<< HEAD
    "from collections import Counter"
=======
    "from collections import Counter\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some hyperparameters\n",
    "\n",
    "language = \"english\"\n",
    "# language = \"slovenian\"\n",
    "# language = \"romanian\"\n",
    "\n",
    "# comparison = \"within-1\"\n",
    "# comparison = \"within-2\"\n",
    "comparison = \"between\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if language==\"english\":\n",
    "    # GUM (Georgetown University Multilayer) corpus from UD website\n",
    "    corpus1_dir = \"data/ud-en-gum/\"\n",
    "    train1 = corpus1_dir+\"en_gum-ud-train.conllu\"\n",
    "    dev1 = corpus1_dir+\"en_gum-ud-dev.conllu\"\n",
    "\n",
    "    # wsj corpus with different conventions\n",
    "    # converted from Stanford dependencies (?)\n",
    "    corpus2_dir = \"data/wsj-DIFF-CONVENTIONS/\"\n",
    "    train2 = corpus2_dir+\"train.conllu\"\n",
    "    dev2 = corpus2_dir+\"dev.conllu\"\n",
    "    test2 = corpus2_dir+\"test.conllu\""
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select language\n",
    "language = \"english\"\n",
    "# language = \"persian\"\n",
    "\n",
    "# Select number of sentences for each corpus\n",
    "num_sents = 250\n",
    "\n",
    "# Select number of similar words to create new word pairs\n",
    "top_n = 10\n",
    "# threshold = [0.95, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0,0]\n",
    "threshold = 0.0\n",
    "\n",
    "# Select seed for training data (1-5)\n",
    "seed = 3\n",
    "\n",
    "BorC = \"B\"\n",
    "# BorC = \"C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpora"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if language==\"slovenian\":\n",
    "    # SSJ corpus\n",
    "    corpus1_dir = \"data/UD_Slovenian-SSJ/\"\n",
    "    train1 = corpus1_dir+\"sl_ssj-ud-train.conllu\"\n",
    "    dev1 = corpus1_dir+\"sl_ssj-ud-dev.conllu\"\n",
    "\n",
    "    # SST corpus\n",
    "    corpus2_dir = \"data/UD_Slovenian-SST/\"\n",
    "    train2 = corpus2_dir+\"sl_sst-ud-train.conllu\"\n",
    "    dev2 = corpus2_dir+\"sl_sst-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if language==\"romanian\":\n",
    "    # RRT corpus\n",
    "    corpus1_dir = \"data/UD_Romanian-RRT/\"\n",
    "    train1 = corpus1_dir+\"ro_rrt-ud-train.conllu\"\n",
    "    dev1 = corpus1_dir+\"ro_rrt-ud-dev.conllu\"\n",
    "\n",
    "    # Nonstandard corpus\n",
    "    corpus2_dir = \"data/UD_Romanian-Nonstandard/\"\n",
    "    train2 = corpus2_dir+\"ro_nonstandard-ud-train.conllu\"\n",
    "    dev2 = corpus2_dir+\"ro_nonstandard-ud-dev.conllu\""
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_A, corpus_B = f.load_corpus(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vectors"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gum_relations = []\n",
    "wsj_relations = []"
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./glove.840B.300d.magnitude\n",
      "vectors/english_corpus=A_sents=250_seed=3.magnitude\n"
     ]
    }
   ],
   "source": [
    "vectors = f.load_vectors(language)\n",
    "# vector_type = \"fastText\"\n",
    "vector_type = \"GloVe\"\n",
    "bert_vectors = \"vectors/\"+language+\"_corpus=A_sents=\"+str(num_sents)+\"_seed=\"+str(seed)+\".magnitude\"\n",
    "print(vectors)\n",
    "print(bert_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get word-word-relation triples for each corpus"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ud_data(ud_file):\n",
    "    \"\"\"\n",
    "    Processes UD data into dictionaries of word pairs, relations, and sentences.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing '{ud_file}' data file...\")\n",
    "    with open(ud_file) as infile:\n",
    "        ud_lines = infile.readlines()\n",
    "    # get list of sentences\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in ud_lines:\n",
    "#         print(line)\n",
    "        if \"# sent_id = \" in line:\n",
    "            sentence.append(line.split(\" sent_id = \")[1].strip())\n",
    "        if \"# text =\" in line:\n",
    "            sentence.append(line.split(\" text =\")[1].strip())\n",
    "        elif line[0] == \"#\":\n",
    "            continue\n",
    "        elif len(line.strip()) == 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            split = line.split(\"\\t\")\n",
    "            sentence.append(split)\n",
    "\n",
    "    pair_to_relations = {}\n",
    "    pair_relation_to_sentences = {}\n",
    "    for sentence in tqdm(sentences):\n",
    "#         print(\"\\nSENTENCE:\\t\",sentence)\n",
    "        # each sentence is a list of lines split on tabs\n",
    "        sentence_id = sentence[0]\n",
    "        sentence_text = sentence[1]\n",
    "#         print(\"Sentence ID:\\t\"+sentence_id)\n",
    "#         print(\"Sentence Text:\\t\"+sentence_text)\n",
    "        for line in sentence[2:]:\n",
    "#             print(\"LINE:\\t\",line)\n",
    "            word_idx = line[0]\n",
    "#             print(\"word idx:\\t\",word_idx)\n",
    "            word = line[1]\n",
    "#             print(\"word:\\t\"+word)\n",
    "            head_idx = int(line[6])\n",
    "#             print(\"head idx:\\t\",head_idx)\n",
    "#             head_idx+1 needed to match place in list\n",
    "            head_word = sentence[head_idx+1][1] if head_idx != 0 else \"#ROOT#\"\n",
    "#             print(\"head word:\\t\"+head_word)\n",
    "            relation = line[7]\n",
    "            if relation not in gum_relations:\n",
    "                gum_relations.append(relation)\n",
    "#             print(\"relation:\\t\"+relation)\n",
    "            word_pair = (head_word, word)\n",
    "#             print(\"word pair:\\t\",word_pair)\n",
    "            if word_pair in pair_to_relations:\n",
    "                pair_to_relations[word_pair].append(relation)\n",
    "            else:\n",
    "                pair_to_relations[word_pair] = [relation]\n",
    "            pair_relation = (word_pair, relation)\n",
    "            if pair_relation in pair_relation_to_sentences:\n",
    "                pair_relation_to_sentences[pair_relation].append([sentence_id, sentence_text])\n",
    "            else:\n",
    "                pair_relation_to_sentences[pair_relation] = [[sentence_id, sentence_text]]\n",
    "    \"\"\"\n",
    "    pair_to_relations dict:\n",
    "        e.g. {(\"Need\", \"'ll\"): [\"aux\"], ...}\n",
    "\n",
    "    pair_relation_to_sentences dict:\n",
    "        e.g. {((\"funnel\", \"Large\"), \"amod\"): [\"Large funnel or strainer to hold filter\"], ...}\n",
    "    \"\"\"\n",
    "    print(f\"\\t{len(pair_to_relations):,} head-dependent:relation pairs\")\n",
    "    print(f\"\\t{len(pair_relation_to_sentences):,} head-dependent-relation:sentence triples\")\n",
    "    return pair_to_relations, pair_relation_to_sentences"
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_A = \"train_data/en/\"+language+\"_train_corpus=A_sents=\"+str(num_sents)+\"_seed=\"+str(seed)+\".txt\"\n",
    "corpus_B = \"train_data/en/\"+language+\"_train_corpus=\"+BorC+\"_sents=\"+str(num_sents)+\"_seed=\"+str(seed)+\".txt\""
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wsj_data(ud_file):\n",
    "    \"\"\"\n",
    "    Processes UD data into dictionaries of word pairs, relations, and sentences.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing '{ud_file}' data file...\")\n",
    "    with open(ud_file) as infile:\n",
    "        ud_lines = infile.readlines()\n",
    "    # get list of sentences\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in ud_lines:\n",
    "        if \"# sent_id = \" in line:\n",
    "            sentence.append(line.split(\" sent_id = \")[1].strip())\n",
    "        if \"# text =\" in line:\n",
    "            sentence.append(line.split(\" text = \")[1].strip())\n",
    "        elif line[0] == \"#\":\n",
    "            continue\n",
    "        elif len(line.strip()) == 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            split = line.split(\"\\t\")\n",
    "            sentence.append(split)\n",
    "\n",
    "    pair_to_relations = {}\n",
    "    pair_relation_to_sentences = {}\n",
    "    for sentence in tqdm(sentences):\n",
    "#         print(\"\\nSENTENCE:\\t\",sentence)\n",
    "        # each sentence is a list of lines split on tabs\n",
    "        sentence_id = sentences.index(sentence)\n",
    "#         print(\"Sentence ID:\\t\"+str(sentence_id))\n",
    "        sentence_text = []\n",
    "        for line in sentence:\n",
    "            word = line[1]\n",
    "            sentence_text.append(word)\n",
    "#         print(\"Sentence Text:\\t\"+\" \".join(sentence_text))\n",
    "        for line in sentence:\n",
    "#             print(\"LINE:\\t\",line)\n",
    "            word_idx = line[0]\n",
    "#             print(\"word idx:\\t\",word_idx)\n",
    "            word = line[1]\n",
    "#             print(\"word:\\t\"+word)\n",
    "            head_idx = int(line[6])\n",
    "#             print(\"head idx:\\t\",head_idx)\n",
    "#             head_idx+1 needed to match place in list\n",
    "            head_word = sentence[head_idx-1][1] if head_idx != 0 else \"#ROOT#\"\n",
    "#             print(\"head word:\\t\"+head_word)\n",
    "            relation = line[7]\n",
    "            if relation not in wsj_relations:\n",
    "                wsj_relations.append(relation)\n",
    "#             print(\"relation:\\t\"+relation)\n",
    "            word_pair = (head_word, word)\n",
    "#             print(\"word pair:\\t\",word_pair)\n",
    "            if word_pair in pair_to_relations:\n",
    "                pair_to_relations[word_pair].append(relation)\n",
    "            else:\n",
    "                pair_to_relations[word_pair] = [relation]\n",
    "            pair_relation = (word_pair, relation)\n",
    "            if pair_relation in pair_relation_to_sentences:\n",
    "                pair_relation_to_sentences[pair_relation].append([sentence_id, \" \".join(sentence_text)])\n",
    "            else:\n",
    "                pair_relation_to_sentences[pair_relation] = [[sentence_id, \" \".join(sentence_text)]]\n",
    "    \"\"\"\n",
    "    pair_to_relations dict:\n",
    "        e.g. {(\"Need\", \"'ll\"): [\"aux\"], ...}\n",
    "\n",
    "    pair_relation_to_sentences dict:\n",
    "        e.g. {((\"funnel\", \"Large\"), \"amod\"): [\"Large funnel or strainer to hold filter\"], ...}\n",
    "    \"\"\"\n",
    "    print(f\"\\t{len(pair_to_relations):,} head-dependent:relation pairs\")\n",
    "    print(f\"\\t{len(pair_relation_to_sentences):,} head-dependent-relation:sentence triples\")\n",
    "    return pair_to_relations, pair_relation_to_sentences"
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 sentences processed\n",
      "4,677 tokens processed\n",
      "Average sentence length:\t18.708\n",
      "4,438 head-dependent:relation pairs\n",
      "4,447 head-dependent-relation:sentence triples\n"
     ]
    }
   ],
   "source": [
    "list_A, sentences_A = f.process_training_data(corpus_A)"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if comparison == \"within-1\":\n",
    "    train_file = train1\n",
    "    dev_file = dev1\n",
    "    train_output = language+\"_train_mismatches_within_corpus1\"+\".tsv\"\n",
    "    dev_output = language+\"_dev_mismatches_within_corpus1\"+\".tsv\"\n",
    "    # get list for training data\n",
    "    train_list, train_sentences = process_ud_data(train_file)\n",
    "    # get list for testing data\n",
    "    dev_list, dev_sentences = process_ud_data(dev_file)\n",
    "    \n",
    "elif comparison == \"within-2\":\n",
    "    train_file = train2\n",
    "    dev_file = dev2\n",
    "    train_output = language+\"_train_mismatches_within_corpus2\"+\".tsv\"\n",
    "    dev_output = language+\"_dev_mismatches_within_corpus_2\"+\".tsv\"\n",
    "    # get list for training data\n",
    "    train_list, train_sentences = process_ud_data(train_file)\n",
    "    # get list for testing data\n",
    "    dev_list, dev_sentences = process_ud_data(dev_file)\n",
    "    ## FOR ENGLISH ONLY (?)\n",
    "    # get list for training data\n",
    "#     train_list, train_sentences = process_wsj_data(train_file)\n",
    "    # get list for testing data\n",
    "#     dev_list, dev_sentences = process_wsj_data(dev_file)\n",
    "\n",
    "    \n",
    "elif comparison == \"between\":\n",
    "#     train_file = wsj_train\n",
    "#     dev_file = gum_train\n",
    "    train_file = train1\n",
    "    dev_file = train2\n",
    "    train_output = language+\"_train_mismatches_between_corpora\"+\".tsv\"\n",
    "    dev_output = language+\"_dev_mismatches_between_corpora\"+\".tsv\"\n",
    "    # get list for training data\n",
    "    train_list, train_sentences = process_ud_data(train_file)\n",
    "    # get list for testing data\n",
    "    # need to keep process_wsj_data for now for English data\n",
    "    dev_list, dev_sentences = process_wsj_data(dev_file)\n",
    "#     dev_list, dev_sentences = process_ud_data(dev_file)"
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 sentences processed\n",
      "6,333 tokens processed\n",
      "Average sentence length:\t25.332\n",
      "5,704 head-dependent:relation pairs\n",
      "5,723 head-dependent-relation:sentence triples\n"
     ]
    }
   ],
   "source": [
    "list_B, sentences_B = f.process_training_data(corpus_B)"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for triple in train_sentences:\n",
    "# #     print(triple, train_sentences[triple])\n",
    "#     for sentence in train_sentences[triple]:\n",
    "#         print(triple, sentence)"
=======
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_A = language+\"_A_mismatches\"+\".tsv\"\n",
    "# output_B = language+\"_B_mismatches\"+\".tsv\"\n",
    "# get list for corpus A data\n",
    "# list_A, sentences_A = f.process_ud_data(corpus_A, num_sents_A)\n",
    "# get list for corpus B data\n",
    "# need to keep process_wsj_data for now for English data\n",
    "# list_B, sentences_B = f.process_wsj_data(corpus_B, num_sents_B)\n",
    "# list_B, sentences_B = f.process_ud_data(corpus_B, num_sents_B)"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pair in dev_list:\n",
    "#     print(pair, dev_list[pair])"
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for triple in sentences_A:\n",
    "#     print(triple, sentences_A[triple])\n",
    "# for sentence in sentences_A[triple]:\n",
    "#     print(triple, sentence)"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for triple in dev_sentences:\n",
    "# #     print(triple, dev_sentences[triple])\n",
    "#     for sentence in dev_sentences[triple]:\n",
=======
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for triple in sentences_B:\n",
    "# #     print(triple, sentences_B[triple])\n",
    "#     for sentence in sentences_B[triple]:\n",
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
    "#         print(triple, sentence)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the two lists\n",
    "dev_mismatches = {}\n",
    "train_mismatches = {}\n",
    "for dev_pair in dev_list.keys():\n",
    "    # If the (head, dependent) pair in dev is in train\n",
    "    if dev_pair in train_list.keys():\n",
    "        # get the relations for that pair in dev\n",
    "        dev_relations = dev_list[dev_pair]\n",
    "#         print(\"dev pair:\\t\\t\",dev_pair)\n",
    "#         print(\"dev relations:\\t\\t\",dev_relations)\n",
    "        # and in train\n",
    "        train_relations = train_list[dev_pair]\n",
    "#         print(\"train relations:\\t\",train_relations)\n",
    "        # TODO: decide which ones we actually care about...\n",
    "        # get the relations in dev NOT in train\n",
    "        not_in_train = [x for x in dev_relations if x not in set(train_relations)]\n",
    "        # and get the relations in train NOT in dev\n",
    "        not_in_dev = [x for x in train_relations if x not in set(dev_relations)]\n",
    "#         print(\"relations not in dev:\\t\",not_in_train)\n",
    "#         print(\"relations not in train:\\t\",not_in_dev)\n",
    "        # if there are relations not in dev/train, add entry to dev_mismatches or train_mismatches for that pair-relation combo\n",
    "        if len(not_in_train) != 0:\n",
    "#             dev_mismatches[dev_pair] = list(set(not_in_train))\n",
    "            dev_mismatches[dev_pair] = not_in_train\n",
    "#             print(\"dev_mismatches:\\t\",dev_mismatches)\n",
    "        if len(not_in_dev) != 0:\n",
    "#             train_mismatches[dev_pair] = list(set(not_in_dev))\n",
    "            train_mismatches[dev_pair] = not_in_dev"
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare triples from each corpus to find mismatched relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches_A = {}\n",
    "mismatches_B = {}\n",
    "for pair_B in list_B.keys():\n",
    "    # If the (head, dependent) pair is in both corpus A and B\n",
    "    if pair_B in list_A.keys():\n",
    "        # get the relations for that pair in B and in A\n",
    "        relations_B = list_B[pair_B]\n",
    "        relations_A = list_A[pair_B]\n",
    "        # TODO: decide which ones we actually care about...\n",
    "        # get the relations in B NOT in A and relations in A NOT in B\n",
    "        not_in_A = [x for x in relations_B if x not in set(relations_A)]\n",
    "        not_in_B = [x for x in relations_A if x not in set(relations_B)]\n",
    "        # if there are relations not in A/B,\n",
    "        # add entry to mismatches_A or mismatches_B for that pair-relation combo\n",
    "        if len(not_in_A) != 0:\n",
    "            mismatches_B[pair_B] = not_in_A\n",
    "        if len(not_in_B) != 0:\n",
    "            mismatches_A[pair_B] = not_in_B"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"{len(dev_mismatches)} pairs with a relation in dev but not in train\")\n",
    "print(f\"{len(train_mismatches)} pairs with a relation in train but not in dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_mismatches"
=======
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 pairs with a relation in corpus A but not in corpus B\n",
      "13 pairs with a relation in corpus B but not in corpus A\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(mismatches_A)} pairs with a relation in corpus A but not in corpus B\")\n",
    "print(f\"{len(mismatches_B)} pairs with a relation in corpus B but not in corpus A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate conversion dictionaries and spreadsheet for analysis"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mismatches"
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_A_simple = f.get_conversions_simple(mismatches_A, \n",
    "                                               sentences_A, \n",
    "                                               list_A, \n",
    "                                               list_B)"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_human_readable_output(filename, mismatches, sentences, this_data, other_data):\n",
    "    \"\"\"\n",
    "    Generates human-readable output file for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        filename: the output filename\n",
    "        \n",
    "        mismatches: the dictionary of word pairs with a relation in one file but not the other\n",
    "        \n",
    "        sentences: the dictionary of word pairs, their relations, and the sentences they're found in\n",
    "    \"\"\"\n",
    "    conversion_dict = {}\n",
    "    with open(filename, \"w\") as output:\n",
    "        header = (\"ID\" + \"\\t\" +\n",
    "                  \"SENTENCE\" + \"\\t\" + \n",
    "                  \"HEAD WORD\" + \"\\t\" + \n",
    "                  \"DEPENDENT WORD\" + \"\\t\" + \n",
    "                  \"RELATION\" + \"\\t\" + \n",
    "                  \"TOP RELATION IN THIS DATA\" + \"\\t\" +\n",
    "                  \"COUNT OF TOP RELATION IN THIS DATA\" + \"\\t\" +\n",
    "                  \"PROPORTION OF TOP RELATION IN THIS DATA\" + \"\\t\"\n",
    "                  \"TOP RELATION IN OTHER DATA\" + \"\\t\" +\n",
    "                  \"COUNT OF TOP RELATION IN OTHER DATA\" + \"\\t\" +\n",
    "                  \"PROPORTION OF TOP RELATION IN OTHER DATA\" + \"\\n\")\n",
    "        output.write(header)\n",
    "        for pair in mismatches:\n",
    "#             print(\"word pair:\\t\", pair)\n",
    "            head_word = pair[0]\n",
    "            dependent_word = pair[1]\n",
    "            # relations for this pair in this partition / corpus\n",
    "            these_relations = Counter(this_data[pair])\n",
    "            # relations for this pair in other partition / corpus\n",
    "            other_relations = Counter(other_data[pair])\n",
    "#             print(\"other_relations:\\t\", sum(Counter(other_relations).values()))\n",
    "            for relation in list(set(mismatches[pair])):\n",
    "                triple = (pair, relation)\n",
    "                sentence_ids = []\n",
    "                sentence_texts = []\n",
    "                for sentence in sentences[triple]:\n",
    "                    sentence_id = str(sentence[0])\n",
    "                    sentence_text = sentence[1]\n",
    "                    sentence_ids.append(sentence_id)\n",
    "                    sentence_texts.append(sentence_text)\n",
    "                # get most common label/count for this data and other data\n",
    "                most_common_label = these_relations.most_common(1)[0][0]\n",
    "                most_common_count = these_relations.most_common(1)[0][1]\n",
    "                sum_these_relations = sum(these_relations.values())\n",
    "                most_common_label_other = other_relations.most_common(1)[0][0]\n",
    "                most_common_count_other = other_relations.most_common(1)[0][1]\n",
    "                sum_other_relations = sum(other_relations.values())\n",
    "                line = (\"('\"+(\"', '\").join(sentence_ids)+\"')\" + \"\\t\" +\n",
    "                        \"('\"+(\"', '\").join(sentence_texts)+\"')\" + \"\\t\" +\n",
    "                        head_word + \"\\t\" +\n",
    "                        dependent_word + \"\\t\" +\n",
    "                        relation + \"\\t\" +\n",
    "                        most_common_label + \"\\t\" +\n",
    "                        str(most_common_count) + \"\\t\" +\n",
    "                        str(most_common_count/sum_these_relations) + \"\\t\" +\n",
    "                        most_common_label_other + \"\\t\" +\n",
    "                        str(most_common_count_other) + \"\\t\" +\n",
    "                        str(most_common_count_other/sum_other_relations)+ \"\\n\"    \n",
    "                        )\n",
    "                key = (head_word, dependent_word, relation)\n",
    "                value = {\"most_common_label\": most_common_label, \n",
    "                         \"most_common_count\": most_common_count, \n",
    "                         \"proportion_these\": most_common_count/sum_these_relations, \n",
    "                         \"most_common_label_other\": most_common_label_other, \n",
    "                         \"most_common_count_other\": most_common_count_other, \n",
    "                         \"proportion_other\": most_common_count_other/sum_other_relations}\n",
    "                conversion_dict[key] = value\n",
    "                print(\"LINE:\\t\"+line)\n",
    "                output.write(line)\n",
    "    return conversion_dict"
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_B_simple = f.get_conversions_simple(mismatches_B, \n",
    "                                               sentences_B, \n",
    "                                               list_B, \n",
    "                                               list_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create human-readable spreadsheet for debugging"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_conversion = generate_human_readable_output(dev_output, dev_mismatches, dev_sentences, dev_list, train_list)"
=======
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_readable_A_simple = \"human_readable/\"+corpus_A[11:-4]+\"_simple_human_readable.tsv\"\n",
    "# human_readable_B_simple = \"human_readable/\"+corpus_B[11:-4]+\"_simple_human_readable.tsv\""
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conversion = generate_human_readable_output(train_output, train_mismatches, train_sentences, train_list, dev_list)"
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.generate_human_readable_output(human_readable_A_simple, mismatches_A, sentences_A, list_A, list_B)\n",
    "# f.generate_human_readable_output(human_readable_B_simple, mismatches_B, sentences_B, list_B, list_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create converted conllu files for corpus A and corpus B"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_conversion[('role', 'played', 'acl')]"
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted_corpus_A_simple = corpus_A[:-4]+\"_converted_simple.conllu\"\n",
    "converted_corpus_B_simple = corpus_B[:-4]+\"_converted_simple.conllu\""
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conversion"
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.apply_conversions(corpus_A, converted_corpus_A_simple, conversion_A_simple)\n",
    "f.apply_conversions(corpus_B, converted_corpus_B_simple, conversion_B_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate conversion dictionaries and spreadsheet for analysis"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_conversions(input_file, output_file, conversion_dictionary):\n",
    "    with open(input_file) as infile:\n",
    "        lines = infile.readlines()\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    metadata = []\n",
    "    for line in lines:\n",
    "        if line[0] == \"#\":\n",
    "            metadata.append(line)\n",
    "            if \"# text =\" in line:\n",
    "                sentence.append(line.split(\" text = \")[1].strip())\n",
    "        elif len(line.strip()) == 0:\n",
    "            item = [metadata]\n",
    "            item.append(sentence)\n",
    "            sentences.append(item)\n",
    "            metadata = []\n",
    "            sentence = []\n",
    "        else:\n",
    "            split = line.split(\"\\t\")\n",
    "            sentence.append(split)\n",
    "    for sentence in sentences:\n",
    "        metadata = sentence[0]\n",
    "        the_rest = sentence[1]\n",
    "        sentence_text = sentence[1][0]\n",
    "        for sent in sentence[1][1:]:\n",
    "#             print(sent)\n",
    "            idx = sent[0]\n",
    "            word = sent[1]\n",
    "            lemma = sent[2]\n",
    "            head_idx = int(sent[6])\n",
    "            head_word = sentence[1][head_idx][1] if head_idx != 0 else \"#ROOT#\"\n",
    "            relation = sent[7]\n",
    "            triple = (head_word, word, relation)\n",
    "            if conversion_dictionary.get(triple):\n",
    "#                 print(\"\\n\")\n",
    "#                 print(triple)\n",
    "#                 print(conversion_dictionary.get(triple))\n",
    "                new_relation = conversion_dictionary[triple][\"most_common_label_other\"]\n",
    "#                 print(\"OLD:\\t\"+relation)\n",
    "#                 print(\"NEW:\\t\"+new_relation)\n",
    "#                 print(sent)\n",
    "                sent[7] = new_relation\n",
    "#                 print(sent)\n",
    "            else:\n",
    "                continue\n",
    "#         with open(output_file, \"a\") as outfile:\n",
    "#             for line in metadata:\n",
    "#                 outfile.write(line)\n",
    "#             for sent in sentence[1][1:]:\n",
    "# #                 print(sent)\n",
    "#                 outfile.write(\"\\t\".join(sent))\n",
    "#             outfile.write(\"\\n\")\n",
    "            "
=======
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh = 0.0"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsj conversion needed because English WSJ data does not have metadata lines, so it messes up the indexing"
=======
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for thresh in threshold:\n",
    "#     conversion_A_pretrained = f.get_conversions_pretrained(mismatches_A, \n",
    "#                                                            sentences_A, \n",
    "#                                                            list_A, \n",
    "#                                                            list_B,\n",
    "#                                                            vectors,\n",
    "#                                                            top_n,\n",
    "#                                                            thresh)\n",
    "#     converted_corpus_A_pretrained = corpus_A[:-4]+\"_converted_pretrained=\"+vector_type+\"_thresh=\"+str(thresh)+\".conllu\"\n",
    "#     f.apply_conversions(corpus_A, converted_corpus_A_pretrained, conversion_A_pretrained)"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wsj_conversions(input_file, conversion_dictionary, threshold):\n",
    "    \n",
    "    output_file = input_file+\"_converted.conllu\"\n",
    "    with open(input_file) as infile:\n",
    "        lines = infile.readlines()\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    metadata = []\n",
    "    for line in lines:\n",
    "        if line[0] == \"#\":\n",
    "            metadata.append(line)\n",
    "            if \"# text =\" in line:\n",
    "                sentence.append(line.split(\" text = \")[1].strip())\n",
    "        elif len(line.strip()) == 0:\n",
    "            item = [metadata]\n",
    "            item.append(sentence)\n",
    "            sentences.append(item)\n",
    "            metadata = []\n",
    "            sentence = []\n",
    "        else:\n",
    "            split = line.split(\"\\t\")\n",
    "            sentence.append(split)\n",
    "    for sentence in sentences:\n",
    "        metadata = sentence[0]\n",
    "        the_rest = sentence[1]\n",
    "        sentence_text = sentence[1][0]\n",
    "        for sent in sentence[1]:\n",
    "#             print(sent)\n",
    "            idx = sent[0]\n",
    "            word = sent[1]\n",
    "            lemma = sent[2]\n",
    "            head_idx = int(sent[6])\n",
    "            head_word = sentence[1][head_idx-1][1] if head_idx != 0 else \"#ROOT#\"\n",
    "            relation = sent[7]\n",
    "            triple = (head_word, word, relation)\n",
    "#             print(triple)\n",
    "            if conversion_dictionary.get(triple):\n",
    "#                 print(\"\\n\")\n",
    "#                 print(triple)\n",
    "#                 print(conversion_dictionary.get(triple))\n",
    "                new_relation = conversion_dictionary[triple][\"most_common_label_other\"]\n",
    "                other_proportion = conversion_dictionary[triple][\"proportion_other\"]\n",
    "#                 print(\"OLD:\\t\"+relation)\n",
    "#                 print(\"NEW:\\t\"+new_relation)\n",
    "#                 print(sent)\n",
    "#                 print(other_proportion)\n",
    "                if other_proportion >= threshold:\n",
    "                    sent[7] = new_relation\n",
    "#                 print(sent)\n",
    "            else:\n",
    "                continue\n",
    "#         with open(output_file, \"a\") as outfile:\n",
    "#             for line in metadata:\n",
    "#                 outfile.write(line)\n",
    "#             for sent in sentence[1]:\n",
    "# #                 print(sent)\n",
    "#                 outfile.write(\"\\t\".join(sent))\n",
    "#             outfile.write(\"\\n\")\n",
    "            "
=======
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_B_pretrained = f.get_conversions_pretrained(mismatches_B, \n",
    "                                                       sentences_B, \n",
    "                                                       list_B, \n",
    "                                                       list_A,\n",
    "                                                       vectors,\n",
    "                                                       top_n,\n",
    "                                                       threshold)\n",
    "converted_corpus_B_pretrained = corpus_B[:-4]+\"_converted_pretrained=\"+vector_type+\".conllu\"\n",
    "f.apply_conversions(corpus_B, converted_corpus_B_pretrained, conversion_B_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create converted files for corpus A and corpus B"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_train_file = train_file+\"_converted.conllu\"\n",
    "converted_dev_file = dev_file+\"_converted.conllu\"\n",
    "print(converted_train_file)"
=======
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted_corpus_A_pretrained = corpus_A[:-4]+\"_converted_pretrained=\"+vector_type+\".conllu\"\n",
    "# converted_corpus_B_pretrained = corpus_B[:-4]+\"_converted_pretrained=\"+vector_type+\".conllu\""
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_conversions(train_file, converted_train_file, train_conversion)"
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.apply_conversions(corpus_A, converted_corpus_A_pretrained, conversion_A_pretrained)\n",
    "# f.apply_conversions(corpus_B, converted_corpus_B_pretrained, conversion_B_pretrained)"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_wsj_conversions(dev_file, dev_conversion, 0.5)"
=======
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_B_BERT = f.get_conversions_pretrained(mismatches_B, \n",
    "                                                       sentences_B, \n",
    "                                                       list_B, \n",
    "                                                       list_A,\n",
    "                                                       vectors,\n",
    "                                                       top_n,\n",
    "                                                       threshold)\n",
    "converted_corpus_B_BERT = corpus_B[:-4]+\"_converted_BERT.conllu\"\n",
    "f.apply_conversions(corpus_B, converted_corpus_B_BERT, conversion_B_BERT)"
>>>>>>> a59557d98823307973d76b84d4aa8d2a6c7d2ade
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
