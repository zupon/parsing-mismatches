{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some hyperparameters\n",
    "\n",
    "language = \"english\"\n",
    "# language = \"slovenian\"\n",
    "# language = \"romanian\"\n",
    "\n",
    "# comparison = \"within-1\"\n",
    "# comparison = \"within-2\"\n",
    "comparison = \"between\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if language==\"english\":\n",
    "    # GUM (Georgetown University Multilayer) corpus from UD website\n",
    "    corpus1_dir = \"data/ud-en-gum/\"\n",
    "    train1 = corpus1_dir+\"en_gum-ud-train.conllu\"\n",
    "    dev1 = corpus1_dir+\"en_gum-ud-dev.conllu\"\n",
    "\n",
    "    # wsj corpus with different conventions\n",
    "    # converted from Stanford dependencies (?)\n",
    "    corpus2_dir = \"data/wsj-DIFF-CONVENTIONS/\"\n",
    "    train2 = corpus2_dir+\"train.conllu\"\n",
    "    dev2 = corpus2_dir+\"dev.conllu\"\n",
    "    test2 = corpus2_dir+\"test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if language==\"slovenian\":\n",
    "    # SSJ corpus\n",
    "    corpus1_dir = \"data/UD_Slovenian-SSJ/\"\n",
    "    train1 = corpus1_dir+\"sl_ssj-ud-train.conllu\"\n",
    "    dev1 = corpus1_dir+\"sl_ssj-ud-dev.conllu\"\n",
    "\n",
    "    # SST corpus\n",
    "    corpus2_dir = \"data/UD_Slovenian-SST/\"\n",
    "    train2 = corpus2_dir+\"sl_sst-ud-train.conllu\"\n",
    "    dev2 = corpus2_dir+\"sl_sst-ud-test.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if language==\"romanian\":\n",
    "    # RRT corpus\n",
    "    corpus1_dir = \"data/UD_Romanian-RRT/\"\n",
    "    train1 = corpus1_dir+\"ro_rrt-ud-train.conllu\"\n",
    "    dev1 = corpus1_dir+\"ro_rrt-ud-dev.conllu\"\n",
    "\n",
    "    # Nonstandard corpus\n",
    "    corpus2_dir = \"data/UD_Romanian-Nonstandard/\"\n",
    "    train2 = corpus2_dir+\"ro_nonstandard-ud-train.conllu\"\n",
    "    dev2 = corpus2_dir+\"ro_nonstandard-ud-dev.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gum_relations = []\n",
    "wsj_relations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ud_data(ud_file):\n",
    "    \"\"\"\n",
    "    Processes UD data into dictionaries of word pairs, relations, and sentences.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing '{ud_file}' data file...\")\n",
    "    with open(ud_file) as infile:\n",
    "        ud_lines = infile.readlines()\n",
    "    # get list of sentences\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in ud_lines:\n",
    "#         print(line)\n",
    "        if \"# sent_id = \" in line:\n",
    "            sentence.append(line.split(\" sent_id = \")[1].strip())\n",
    "        if \"# text =\" in line:\n",
    "            sentence.append(line.split(\" text =\")[1].strip())\n",
    "        elif line[0] == \"#\":\n",
    "            continue\n",
    "        elif len(line.strip()) == 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            split = line.split(\"\\t\")\n",
    "            sentence.append(split)\n",
    "\n",
    "    pair_to_relations = {}\n",
    "    pair_relation_to_sentences = {}\n",
    "    for sentence in tqdm(sentences):\n",
    "#         print(\"\\nSENTENCE:\\t\",sentence)\n",
    "        # each sentence is a list of lines split on tabs\n",
    "        sentence_id = sentence[0]\n",
    "        sentence_text = sentence[1]\n",
    "#         print(\"Sentence ID:\\t\"+sentence_id)\n",
    "#         print(\"Sentence Text:\\t\"+sentence_text)\n",
    "        for line in sentence[2:]:\n",
    "#             print(\"LINE:\\t\",line)\n",
    "            word_idx = line[0]\n",
    "#             print(\"word idx:\\t\",word_idx)\n",
    "            word = line[1]\n",
    "#             print(\"word:\\t\"+word)\n",
    "            head_idx = int(line[6])\n",
    "#             print(\"head idx:\\t\",head_idx)\n",
    "#             head_idx+1 needed to match place in list\n",
    "            head_word = sentence[head_idx+1][1] if head_idx != 0 else \"#ROOT#\"\n",
    "#             print(\"head word:\\t\"+head_word)\n",
    "            relation = line[7]\n",
    "            if relation not in gum_relations:\n",
    "                gum_relations.append(relation)\n",
    "#             print(\"relation:\\t\"+relation)\n",
    "            word_pair = (head_word, word)\n",
    "#             print(\"word pair:\\t\",word_pair)\n",
    "            if word_pair in pair_to_relations:\n",
    "                pair_to_relations[word_pair].append(relation)\n",
    "            else:\n",
    "                pair_to_relations[word_pair] = [relation]\n",
    "            pair_relation = (word_pair, relation)\n",
    "            if pair_relation in pair_relation_to_sentences:\n",
    "                pair_relation_to_sentences[pair_relation].append([sentence_id, sentence_text])\n",
    "            else:\n",
    "                pair_relation_to_sentences[pair_relation] = [[sentence_id, sentence_text]]\n",
    "    \"\"\"\n",
    "    pair_to_relations dict:\n",
    "        e.g. {(\"Need\", \"'ll\"): [\"aux\"], ...}\n",
    "\n",
    "    pair_relation_to_sentences dict:\n",
    "        e.g. {((\"funnel\", \"Large\"), \"amod\"): [\"Large funnel or strainer to hold filter\"], ...}\n",
    "    \"\"\"\n",
    "    print(f\"\\t{len(pair_to_relations):,} head-dependent:relation pairs\")\n",
    "    print(f\"\\t{len(pair_relation_to_sentences):,} head-dependent-relation:sentence triples\")\n",
    "    return pair_to_relations, pair_relation_to_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_wsj_data(ud_file):\n",
    "    \"\"\"\n",
    "    Processes UD data into dictionaries of word pairs, relations, and sentences.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing '{ud_file}' data file...\")\n",
    "    with open(ud_file) as infile:\n",
    "        ud_lines = infile.readlines()\n",
    "    # get list of sentences\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in ud_lines:\n",
    "        if \"# sent_id = \" in line:\n",
    "            sentence.append(line.split(\" sent_id = \")[1].strip())\n",
    "        if \"# text =\" in line:\n",
    "            sentence.append(line.split(\" text = \")[1].strip())\n",
    "        elif line[0] == \"#\":\n",
    "            continue\n",
    "        elif len(line.strip()) == 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            split = line.split(\"\\t\")\n",
    "            sentence.append(split)\n",
    "\n",
    "    pair_to_relations = {}\n",
    "    pair_relation_to_sentences = {}\n",
    "    for sentence in tqdm(sentences):\n",
    "#         print(\"\\nSENTENCE:\\t\",sentence)\n",
    "        # each sentence is a list of lines split on tabs\n",
    "        sentence_id = sentences.index(sentence)\n",
    "#         print(\"Sentence ID:\\t\"+str(sentence_id))\n",
    "        sentence_text = []\n",
    "        for line in sentence:\n",
    "            word = line[1]\n",
    "            sentence_text.append(word)\n",
    "#         print(\"Sentence Text:\\t\"+\" \".join(sentence_text))\n",
    "        for line in sentence:\n",
    "#             print(\"LINE:\\t\",line)\n",
    "            word_idx = line[0]\n",
    "#             print(\"word idx:\\t\",word_idx)\n",
    "            word = line[1]\n",
    "#             print(\"word:\\t\"+word)\n",
    "            head_idx = int(line[6])\n",
    "#             print(\"head idx:\\t\",head_idx)\n",
    "#             head_idx+1 needed to match place in list\n",
    "            head_word = sentence[head_idx-1][1] if head_idx != 0 else \"#ROOT#\"\n",
    "#             print(\"head word:\\t\"+head_word)\n",
    "            relation = line[7]\n",
    "            if relation not in wsj_relations:\n",
    "                wsj_relations.append(relation)\n",
    "#             print(\"relation:\\t\"+relation)\n",
    "            word_pair = (head_word, word)\n",
    "#             print(\"word pair:\\t\",word_pair)\n",
    "            if word_pair in pair_to_relations:\n",
    "                pair_to_relations[word_pair].append(relation)\n",
    "            else:\n",
    "                pair_to_relations[word_pair] = [relation]\n",
    "            pair_relation = (word_pair, relation)\n",
    "            if pair_relation in pair_relation_to_sentences:\n",
    "                pair_relation_to_sentences[pair_relation].append([sentence_id, \" \".join(sentence_text)])\n",
    "            else:\n",
    "                pair_relation_to_sentences[pair_relation] = [[sentence_id, \" \".join(sentence_text)]]\n",
    "    \"\"\"\n",
    "    pair_to_relations dict:\n",
    "        e.g. {(\"Need\", \"'ll\"): [\"aux\"], ...}\n",
    "\n",
    "    pair_relation_to_sentences dict:\n",
    "        e.g. {((\"funnel\", \"Large\"), \"amod\"): [\"Large funnel or strainer to hold filter\"], ...}\n",
    "    \"\"\"\n",
    "    print(f\"\\t{len(pair_to_relations):,} head-dependent:relation pairs\")\n",
    "    print(f\"\\t{len(pair_relation_to_sentences):,} head-dependent-relation:sentence triples\")\n",
    "    return pair_to_relations, pair_relation_to_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if comparison == \"within-1\":\n",
    "    train_file = train1\n",
    "    dev_file = dev1\n",
    "    train_output = language+\"_train_mismatches_within_corpus1\"+\".tsv\"\n",
    "    dev_output = language+\"_dev_mismatches_within_corpus1\"+\".tsv\"\n",
    "    # get list for training data\n",
    "    train_list, train_sentences = process_ud_data(train_file)\n",
    "    # get list for testing data\n",
    "    dev_list, dev_sentences = process_ud_data(dev_file)\n",
    "    \n",
    "elif comparison == \"within-2\":\n",
    "    train_file = train2\n",
    "    dev_file = dev2\n",
    "    train_output = language+\"_train_mismatches_within_corpus2\"+\".tsv\"\n",
    "    dev_output = language+\"_dev_mismatches_within_corpus_2\"+\".tsv\"\n",
    "    # get list for training data\n",
    "    train_list, train_sentences = process_ud_data(train_file)\n",
    "    # get list for testing data\n",
    "    dev_list, dev_sentences = process_ud_data(dev_file)\n",
    "    ## FOR ENGLISH ONLY (?)\n",
    "    # get list for training data\n",
    "#     train_list, train_sentences = process_wsj_data(train_file)\n",
    "    # get list for testing data\n",
    "#     dev_list, dev_sentences = process_wsj_data(dev_file)\n",
    "\n",
    "    \n",
    "elif comparison == \"between\":\n",
    "#     train_file = wsj_train\n",
    "#     dev_file = gum_train\n",
    "    train_file = train1\n",
    "    dev_file = train2\n",
    "    train_output = language+\"_train_mismatches_between_corpora\"+\".tsv\"\n",
    "    dev_output = language+\"_dev_mismatches_between_corpora\"+\".tsv\"\n",
    "    # get list for training data\n",
    "    train_list, train_sentences = process_ud_data(train_file)\n",
    "    # get list for testing data\n",
    "    # need to keep process_wsj_data for now for English data\n",
    "    dev_list, dev_sentences = process_wsj_data(dev_file)\n",
    "#     dev_list, dev_sentences = process_ud_data(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for triple in train_sentences:\n",
    "# #     print(triple, train_sentences[triple])\n",
    "#     for sentence in train_sentences[triple]:\n",
    "#         print(triple, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pair in dev_list:\n",
    "#     print(pair, dev_list[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for triple in dev_sentences:\n",
    "# #     print(triple, dev_sentences[triple])\n",
    "#     for sentence in dev_sentences[triple]:\n",
    "#         print(triple, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the two lists\n",
    "dev_mismatches = {}\n",
    "train_mismatches = {}\n",
    "for dev_pair in dev_list.keys():\n",
    "    # If the (head, dependent) pair in dev is in train\n",
    "    if dev_pair in train_list.keys():\n",
    "        # get the relations for that pair in dev\n",
    "        dev_relations = dev_list[dev_pair]\n",
    "#         print(\"dev pair:\\t\\t\",dev_pair)\n",
    "#         print(\"dev relations:\\t\\t\",dev_relations)\n",
    "        # and in train\n",
    "        train_relations = train_list[dev_pair]\n",
    "#         print(\"train relations:\\t\",train_relations)\n",
    "        # TODO: decide which ones we actually care about...\n",
    "        # get the relations in dev NOT in train\n",
    "        not_in_train = [x for x in dev_relations if x not in set(train_relations)]\n",
    "        # and get the relations in train NOT in dev\n",
    "        not_in_dev = [x for x in train_relations if x not in set(dev_relations)]\n",
    "#         print(\"relations not in dev:\\t\",not_in_train)\n",
    "#         print(\"relations not in train:\\t\",not_in_dev)\n",
    "        # if there are relations not in dev/train, add entry to dev_mismatches or train_mismatches for that pair-relation combo\n",
    "        if len(not_in_train) != 0:\n",
    "#             dev_mismatches[dev_pair] = list(set(not_in_train))\n",
    "            dev_mismatches[dev_pair] = not_in_train\n",
    "#             print(\"dev_mismatches:\\t\",dev_mismatches)\n",
    "        if len(not_in_dev) != 0:\n",
    "#             train_mismatches[dev_pair] = list(set(not_in_dev))\n",
    "            train_mismatches[dev_pair] = not_in_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(f\"{len(dev_mismatches)} pairs with a relation in dev but not in train\")\n",
    "print(f\"{len(train_mismatches)} pairs with a relation in train but not in dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_human_readable_output(filename, mismatches, sentences, this_data, other_data):\n",
    "    \"\"\"\n",
    "    Generates human-readable output file for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        filename: the output filename\n",
    "        \n",
    "        mismatches: the dictionary of word pairs with a relation in one file but not the other\n",
    "        \n",
    "        sentences: the dictionary of word pairs, their relations, and the sentences they're found in\n",
    "    \"\"\"\n",
    "    conversion_dict = {}\n",
    "    with open(filename, \"w\") as output:\n",
    "        header = (\"ID\" + \"\\t\" +\n",
    "                  \"SENTENCE\" + \"\\t\" + \n",
    "                  \"HEAD WORD\" + \"\\t\" + \n",
    "                  \"DEPENDENT WORD\" + \"\\t\" + \n",
    "                  \"RELATION\" + \"\\t\" + \n",
    "                  \"TOP RELATION IN THIS DATA\" + \"\\t\" +\n",
    "                  \"COUNT OF TOP RELATION IN THIS DATA\" + \"\\t\" +\n",
    "                  \"PROPORTION OF TOP RELATION IN THIS DATA\" + \"\\t\"\n",
    "                  \"TOP RELATION IN OTHER DATA\" + \"\\t\" +\n",
    "                  \"COUNT OF TOP RELATION IN OTHER DATA\" + \"\\t\" +\n",
    "                  \"PROPORTION OF TOP RELATION IN OTHER DATA\" + \"\\n\")\n",
    "        output.write(header)\n",
    "        for pair in mismatches:\n",
    "#             print(\"word pair:\\t\", pair)\n",
    "            head_word = pair[0]\n",
    "            dependent_word = pair[1]\n",
    "            # relations for this pair in this partition / corpus\n",
    "            these_relations = Counter(this_data[pair])\n",
    "            # relations for this pair in other partition / corpus\n",
    "            other_relations = Counter(other_data[pair])\n",
    "#             print(\"other_relations:\\t\", sum(Counter(other_relations).values()))\n",
    "            for relation in list(set(mismatches[pair])):\n",
    "                triple = (pair, relation)\n",
    "                sentence_ids = []\n",
    "                sentence_texts = []\n",
    "                for sentence in sentences[triple]:\n",
    "                    sentence_id = str(sentence[0])\n",
    "                    sentence_text = sentence[1]\n",
    "                    sentence_ids.append(sentence_id)\n",
    "                    sentence_texts.append(sentence_text)\n",
    "                # get most common label/count for this data and other data\n",
    "                most_common_label = these_relations.most_common(1)[0][0]\n",
    "                most_common_count = these_relations.most_common(1)[0][1]\n",
    "                sum_these_relations = sum(these_relations.values())\n",
    "                most_common_label_other = other_relations.most_common(1)[0][0]\n",
    "                most_common_count_other = other_relations.most_common(1)[0][1]\n",
    "                sum_other_relations = sum(other_relations.values())\n",
    "                line = (\"('\"+(\"', '\").join(sentence_ids)+\"')\" + \"\\t\" +\n",
    "                        \"('\"+(\"', '\").join(sentence_texts)+\"')\" + \"\\t\" +\n",
    "                        head_word + \"\\t\" +\n",
    "                        dependent_word + \"\\t\" +\n",
    "                        relation + \"\\t\" +\n",
    "                        most_common_label + \"\\t\" +\n",
    "                        str(most_common_count) + \"\\t\" +\n",
    "                        str(most_common_count/sum_these_relations) + \"\\t\" +\n",
    "                        most_common_label_other + \"\\t\" +\n",
    "                        str(most_common_count_other) + \"\\t\" +\n",
    "                        str(most_common_count_other/sum_other_relations)+ \"\\n\"    \n",
    "                        )\n",
    "                key = (head_word, dependent_word, relation)\n",
    "                value = {\"most_common_label\": most_common_label, \n",
    "                         \"most_common_count\": most_common_count, \n",
    "                         \"proportion_these\": most_common_count/sum_these_relations, \n",
    "                         \"most_common_label_other\": most_common_label_other, \n",
    "                         \"most_common_count_other\": most_common_count_other, \n",
    "                         \"proportion_other\": most_common_count_other/sum_other_relations}\n",
    "                conversion_dict[key] = value\n",
    "                print(\"LINE:\\t\"+line)\n",
    "                output.write(line)\n",
    "    return conversion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_conversion = generate_human_readable_output(dev_output, dev_mismatches, dev_sentences, dev_list, train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conversion = generate_human_readable_output(train_output, train_mismatches, train_sentences, train_list, dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_conversion[('role', 'played', 'acl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_conversions(input_file, output_file, conversion_dictionary):\n",
    "    with open(input_file) as infile:\n",
    "        lines = infile.readlines()\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    metadata = []\n",
    "    for line in lines:\n",
    "        if line[0] == \"#\":\n",
    "            metadata.append(line)\n",
    "            if \"# text =\" in line:\n",
    "                sentence.append(line.split(\" text = \")[1].strip())\n",
    "        elif len(line.strip()) == 0:\n",
    "            item = [metadata]\n",
    "            item.append(sentence)\n",
    "            sentences.append(item)\n",
    "            metadata = []\n",
    "            sentence = []\n",
    "        else:\n",
    "            split = line.split(\"\\t\")\n",
    "            sentence.append(split)\n",
    "    for sentence in sentences:\n",
    "        metadata = sentence[0]\n",
    "        the_rest = sentence[1]\n",
    "        sentence_text = sentence[1][0]\n",
    "        for sent in sentence[1][1:]:\n",
    "#             print(sent)\n",
    "            idx = sent[0]\n",
    "            word = sent[1]\n",
    "            lemma = sent[2]\n",
    "            head_idx = int(sent[6])\n",
    "            head_word = sentence[1][head_idx][1] if head_idx != 0 else \"#ROOT#\"\n",
    "            relation = sent[7]\n",
    "            triple = (head_word, word, relation)\n",
    "            if conversion_dictionary.get(triple):\n",
    "#                 print(\"\\n\")\n",
    "#                 print(triple)\n",
    "#                 print(conversion_dictionary.get(triple))\n",
    "                new_relation = conversion_dictionary[triple][\"most_common_label_other\"]\n",
    "#                 print(\"OLD:\\t\"+relation)\n",
    "#                 print(\"NEW:\\t\"+new_relation)\n",
    "#                 print(sent)\n",
    "                sent[7] = new_relation\n",
    "#                 print(sent)\n",
    "            else:\n",
    "                continue\n",
    "#         with open(output_file, \"a\") as outfile:\n",
    "#             for line in metadata:\n",
    "#                 outfile.write(line)\n",
    "#             for sent in sentence[1][1:]:\n",
    "# #                 print(sent)\n",
    "#                 outfile.write(\"\\t\".join(sent))\n",
    "#             outfile.write(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsj conversion needed because English WSJ data does not have metadata lines, so it messes up the indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wsj_conversions(input_file, conversion_dictionary, threshold):\n",
    "    \n",
    "    output_file = input_file+\"_converted.conllu\"\n",
    "    with open(input_file) as infile:\n",
    "        lines = infile.readlines()\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    metadata = []\n",
    "    for line in lines:\n",
    "        if line[0] == \"#\":\n",
    "            metadata.append(line)\n",
    "            if \"# text =\" in line:\n",
    "                sentence.append(line.split(\" text = \")[1].strip())\n",
    "        elif len(line.strip()) == 0:\n",
    "            item = [metadata]\n",
    "            item.append(sentence)\n",
    "            sentences.append(item)\n",
    "            metadata = []\n",
    "            sentence = []\n",
    "        else:\n",
    "            split = line.split(\"\\t\")\n",
    "            sentence.append(split)\n",
    "    for sentence in sentences:\n",
    "        metadata = sentence[0]\n",
    "        the_rest = sentence[1]\n",
    "        sentence_text = sentence[1][0]\n",
    "        for sent in sentence[1]:\n",
    "#             print(sent)\n",
    "            idx = sent[0]\n",
    "            word = sent[1]\n",
    "            lemma = sent[2]\n",
    "            head_idx = int(sent[6])\n",
    "            head_word = sentence[1][head_idx-1][1] if head_idx != 0 else \"#ROOT#\"\n",
    "            relation = sent[7]\n",
    "            triple = (head_word, word, relation)\n",
    "#             print(triple)\n",
    "            if conversion_dictionary.get(triple):\n",
    "#                 print(\"\\n\")\n",
    "#                 print(triple)\n",
    "#                 print(conversion_dictionary.get(triple))\n",
    "                new_relation = conversion_dictionary[triple][\"most_common_label_other\"]\n",
    "                other_proportion = conversion_dictionary[triple][\"proportion_other\"]\n",
    "#                 print(\"OLD:\\t\"+relation)\n",
    "#                 print(\"NEW:\\t\"+new_relation)\n",
    "#                 print(sent)\n",
    "#                 print(other_proportion)\n",
    "                if other_proportion >= threshold:\n",
    "                    sent[7] = new_relation\n",
    "#                 print(sent)\n",
    "            else:\n",
    "                continue\n",
    "#         with open(output_file, \"a\") as outfile:\n",
    "#             for line in metadata:\n",
    "#                 outfile.write(line)\n",
    "#             for sent in sentence[1]:\n",
    "# #                 print(sent)\n",
    "#                 outfile.write(\"\\t\".join(sent))\n",
    "#             outfile.write(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_train_file = train_file+\"_converted.conllu\"\n",
    "converted_dev_file = dev_file+\"_converted.conllu\"\n",
    "print(converted_train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_conversions(train_file, converted_train_file, train_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_wsj_conversions(dev_file, dev_conversion, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
